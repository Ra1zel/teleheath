{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('footInfrontOfOther.avi', fourcc, 20.0, (640,  480))\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Can't receive  (stream end?). Exiting ...\")\n",
    "        break\n",
    "    out.write(frame)\n",
    "    cv2.imshow('frame', frame)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "# Release everything if job is finished\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coordinates(landmarks):\n",
    "    left_shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x ,landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER].y]\n",
    "    left_elbow = [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x , landmarks[mp_pose.PoseLandmark.LEFT_ELBOW].y]\n",
    "    left_hip = [landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x , landmarks[mp_pose.PoseLandmark.LEFT_HIP].y]\n",
    "    left_wrist = [landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].x , landmarks[mp_pose.PoseLandmark.LEFT_WRIST].y]\n",
    "    left_knee = [landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].x , landmarks[mp_pose.PoseLandmark.LEFT_KNEE].y]\n",
    "    left_ankle = [landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].x , landmarks[mp_pose.PoseLandmark.LEFT_ANKLE].y]\n",
    "    right_shoulder =[landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x , landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER].y]\n",
    "    right_elbow = [landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].x , landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW].y]\n",
    "    right_hip = [landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].x , landmarks[mp_pose.PoseLandmark.RIGHT_HIP].y]\n",
    "    right_wrist =[landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].x , landmarks[mp_pose.PoseLandmark.RIGHT_WRIST].y]\n",
    "    right_knee = [landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].x , landmarks[mp_pose.PoseLandmark.RIGHT_KNEE].y]\n",
    "    right_ankle = [landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].x , landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE].y]\n",
    "    left_foot_index = [landmarks[mp_pose.PoseLandmark.LEFT_FOOT_INDEX.value].x , landmarks[mp_pose.PoseLandmark.LEFT_FOOT_INDEX].y]\n",
    "    right_foot_index = [landmarks[mp_pose.PoseLandmark.RIGHT_FOOT_INDEX.value].x , landmarks[mp_pose.PoseLandmark.RIGHT_FOOT_INDEX].y]\n",
    "    return [\n",
    "        [left_shoulder,left_elbow,left_wrist],\n",
    "        [right_shoulder,right_elbow,right_wrist],\n",
    "        [left_hip,left_shoulder,left_elbow],\n",
    "        [right_hip,right_shoulder,right_elbow],\n",
    "        [left_shoulder,left_hip,left_knee],\n",
    "        [right_shoulder,right_elbow,right_knee],\n",
    "        [left_hip,left_knee,left_ankle],\n",
    "        [right_hip,right_knee,right_ankle],\n",
    "        [left_knee,left_ankle,left_foot_index],\n",
    "        [right_knee,right_ankle,right_foot_index],\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "landmarks = ['class']\n",
    "for val in range(1,33+1):\n",
    "    landmarks += ['x{}'.format(val),'y{}'.format(val),'z{}'.format(val),'v{}'.format(val)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angles(a,b,c):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    c = np.array(c)\n",
    "\n",
    "    radians = np.arctan2(c[1] - b[1], c[0] - b[0]) - np.arctan2(a[1] - b[1], a[0] - b[0])\n",
    "    angle = np.abs(radians*180.0/np.pi)\n",
    "\n",
    "    if angle > 180.0:\n",
    "        angle = 360 - angle\n",
    "    return angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_angles(required_coordinates):\n",
    "    results = []\n",
    "    for coordinate in required_coordinates:\n",
    "        results.append(calculate_angles(coordinate[0],coordinate[1],coordinate[2]))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('360.avi')\n",
    "with mp_holistic.pose(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as holistic:\n",
    "  while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      # If loading a video, use 'break' instead of 'continue'.\n",
    "      continue\n",
    "\n",
    "    # To improve performance, optionally mark the image as not writeable to\n",
    "    # pass by reference.\n",
    "    image.flags.writeable = False\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = holistic.process(image)\n",
    "\n",
    "    # Draw landmark annotation on the image.\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.face_landmarks,\n",
    "        mp_holistic.FACEMESH_CONTOURS,\n",
    "        landmark_drawing_spec=None,\n",
    "        connection_drawing_spec=mp_drawing_styles\n",
    "        .get_default_face_mesh_contours_style())\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.pose_landmarks,\n",
    "        mp_holistic.POSE_CONNECTIONS,\n",
    "        landmark_drawing_spec=mp_drawing_styles\n",
    "        .get_default_pose_landmarks_style())\n",
    "    # Flip the image horizontally for a selfie-view display.\n",
    "\n",
    "\n",
    "    k=cv2.waitKey(1)\n",
    "    if k == 117:\n",
    "      export_landmarks(results,'up')\n",
    "    if k == 100:\n",
    "      export_landmarks(results,'down')\n",
    "    cv2.imshow('MediaPipe Holistic', cv2.flip(image, 1))\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "      break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For webcam input:\n",
    "cap = cv2.VideoCapture(\"360.avi\")\n",
    "with mp_pose.Pose(\n",
    "    min_detection_confidence=0.5,\n",
    "    smooth_landmarks = True,\n",
    "    # model_complexity = 2,\n",
    "    min_tracking_confidence=0.5) as pose:\n",
    "  while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      # If loading a video, use 'break' instead of 'continue'.\n",
    "      continue\n",
    "\n",
    "    # To improve performance, optionally mark the image as not writeable to\n",
    "    # pass by reference.\n",
    "    image.flags.writeable = False\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(image)\n",
    "    results_temp = results.pose_landmarks\n",
    "    # print(results_temp[0][1][10:])\n",
    "    # Draw the pose annotation on the image.\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results_temp,\n",
    "        mp_pose.POSE_CONNECTIONS,\n",
    "        landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\n",
    "    # Flip the image horizontally for a selfie-view display.\n",
    "    cv2.imshow('MediaPipe Pose', cv2.flip(image, 1))\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "      break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_landmarks = mp.solutions.pose.PoseLandmark\n",
    "pose_landmarks\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lean Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('lean.pkl','rb') as f:\n",
    "    model1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For webcam input:\n",
    "counter = 0\n",
    "current_stage = ''\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5,min_tracking_confidence=0.5) as holistic:\n",
    "  while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      # If loading a video, use 'break' instead of 'continue'.\n",
    "      continue\n",
    "\n",
    "    # To improve performance, optionally mark the image as not writeable to\n",
    "    # pass by reference.\n",
    "    image.flags.writeable = False\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = holistic.process(image)\n",
    "\n",
    "    # Draw landmark annotation on the image.\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    # mp_drawing.draw_landmarks(\n",
    "    #     image,\n",
    "    #     results.face_landmarks,\n",
    "    #     mp_holistic.FACEMESH_CONTOURS,\n",
    "    #     landmark_drawing_spec=None,\n",
    "    #     connection_drawing_spec=mp_drawing_styles\n",
    "    #     .get_default_face_mesh_contours_style())\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.pose_landmarks,\n",
    "        mp_holistic.POSE_CONNECTIONS,\n",
    "        landmark_drawing_spec=mp_drawing_styles\n",
    "        .get_default_pose_landmarks_style())\n",
    "    \n",
    "    try:\n",
    "        row = np.array([[res.x,res.y,res.z,res.visibility] for res in results.pose_landmarks.landmark]).flatten().tolist()\n",
    "        X = pd.DataFrame([row],columns=landmarks[1:])\n",
    "        body_lean_class = model.predict(X)[0]\n",
    "        body_lean_prob = model.predict_proba(X)[0]\n",
    "        print(body_lean_class,body_lean_prob)\n",
    "\n",
    "        if body_lean_class == 'neutral' and body_lean_prob[body_lean_prob.argmax()] >= 0.7:\n",
    "            current_stage = 'neutral'\n",
    "            # print(\"huuuuuu\")\n",
    "        elif body_lean_class == 'left' and body_lean_prob[body_lean_prob.argmax()] >= 0.7:\n",
    "            current_stage = 'left'\n",
    "        elif body_lean_class == 'right' and body_lean_prob[body_lean_prob.argmax()] >= 0.7:\n",
    "            current_stage = 'right'\n",
    "        \n",
    "        # Get status box\n",
    "        cv2.rectangle(image,(0,0),(250,60),(245,117,16),-1)\n",
    "\n",
    "        #Display class\n",
    "        cv2.putText(image,'CLASS'\n",
    "            ,(95,12),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,0,0),1,cv2.LINE_AA)\n",
    "        cv2.putText(image,body_lean_class.split(' ')[0]\n",
    "        , (90,40),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2,cv2.LINE_AA)\n",
    "\n",
    "        #Display Probability\n",
    "        cv2.putText(image,'PROB',(15,12),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,0,0),1,cv2.LINE_AA)\n",
    "        cv2.putText(image,str(round(body_lean_prob[np.argmax(body_lean_prob)],2))\n",
    "        ,(10,40),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2,cv2.LINE_AA)\n",
    "\n",
    "        # #Display Probability\n",
    "        # cv2.putText(image,'COUNT',(180,12),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,0,0),1,cv2.LINE_AA)\n",
    "        # cv2.putText(image,str(counter),(175,40),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2,cv2.LINE_AA)\n",
    "\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    # Flip the image horizontally for a selfie-view display.\n",
    "    cv2.imshow('MediaPipe Holistic',image ) #cv2.flip(image, 1)\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "      break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stance Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stance.pkl','rb') as f:\n",
    "    model2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For webcam input:\n",
    "counter = 0\n",
    "current_stage = ''\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5,min_tracking_confidence=0.5) as holistic:\n",
    "  while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      # If loading a video, use 'break' instead of 'continue'.\n",
    "      continue\n",
    "\n",
    "    # To improve performance, optionally mark the image as not writeable to\n",
    "    # pass by reference.\n",
    "    image.flags.writeable = False\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = holistic.process(image)\n",
    "\n",
    "    # Draw landmark annotation on the image.\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.face_landmarks,\n",
    "        mp_holistic.FACEMESH_CONTOURS,\n",
    "        landmark_drawing_spec=None,\n",
    "        connection_drawing_spec=mp_drawing_styles\n",
    "        .get_default_face_mesh_contours_style())\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.pose_landmarks,\n",
    "        mp_holistic.POSE_CONNECTIONS,\n",
    "        landmark_drawing_spec=mp_drawing_styles\n",
    "        .get_default_pose_landmarks_style())\n",
    "    \n",
    "    try:\n",
    "        row = np.array([[res.x,res.y,res.z,res.visibility] for res in results.pose_landmarks.landmark]).flatten().tolist()\n",
    "        X = pd.DataFrame([row],columns=landmarks[1:])\n",
    "        body_lean_class = model2.predict(X)[0]\n",
    "        body_lean_prob = model2.predict_proba(X)[0]\n",
    "        print(body_lean_class,body_lean_prob)\n",
    "\n",
    "        if body_lean_class == 'neutral' and body_lean_prob[body_lean_prob.argmax()] >= 0.7:\n",
    "            current_stage = 'neutral'\n",
    "            # print(\"huuuuuu\")\n",
    "        elif body_lean_class == 'left' and body_lean_prob[body_lean_prob.argmax()] >= 0.7:\n",
    "            current_stage = 'left'\n",
    "        elif body_lean_class == 'right' and body_lean_prob[body_lean_prob.argmax()] >= 0.7:\n",
    "            current_stage = 'right'\n",
    "        \n",
    "        # Get status box\n",
    "        cv2.rectangle(image,(0,0),(250,60),(245,117,16),-1)\n",
    "\n",
    "        #Display class\n",
    "        cv2.putText(image,'CLASS'\n",
    "            ,(95,12),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,0,0),1,cv2.LINE_AA)\n",
    "        cv2.putText(image,body_lean_class.split(' ')[0]\n",
    "        , (90,40),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2,cv2.LINE_AA)\n",
    "\n",
    "        #Display Probability\n",
    "        cv2.putText(image,'PROB',(15,12),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,0,0),1,cv2.LINE_AA)\n",
    "        cv2.putText(image,str(round(body_lean_prob[np.argmax(body_lean_prob)],2))\n",
    "        ,(10,40),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2,cv2.LINE_AA)\n",
    "\n",
    "        # #Display Probability\n",
    "        # cv2.putText(image,'COUNT',(180,12),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,0,0),1,cv2.LINE_AA)\n",
    "        # cv2.putText(image,str(counter),(175,40),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2,cv2.LINE_AA)\n",
    "\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    # Flip the image horizontally for a selfie-view display.\n",
    "    cv2.imshow('MediaPipe Holistic',image ) #cv2.flip(image, 1)\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "      break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the MediaPipe Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('deadlift.pkl','rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks = ['class']\n",
    "for val in range(1,33+1):\n",
    "    landmarks += ['x{}'.format(val),'y{}'.format(val),'z{}'.format(val),'v{}'.format(val)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For webcam input:\n",
    "counter = 0\n",
    "current_stage = ''\n",
    "current_lean_status = ''\n",
    "prev_lean_status = 'neutral'\n",
    "current_stance_status = ''\n",
    "prev_stance_status = 'normal'\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('final_demo.avi', fourcc, 20.0, (640,480))\n",
    "\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5,min_tracking_confidence=0.5) as holistic:\n",
    "  while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      # If loading a video, use 'break' instead of 'continue'.\n",
    "      continue\n",
    "\n",
    "    # To improve performance, optionally mark the image as not writeable to\n",
    "    # pass by reference.\n",
    "    image.flags.writeable = False\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = holistic.process(image)\n",
    "\n",
    "    # Draw landmark annotation on the image.\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    # mp_drawing.draw_landmarks(\n",
    "    #     image,\n",
    "    #     results.face_landmarks,\n",
    "    #     mp_holistic.FACEMESH_CONTOURS,\n",
    "    #     landmark_drawing_spec=None,\n",
    "    #     connection_drawing_spec=mp_drawing_styles\n",
    "    #     .get_default_face_mesh_contours_style())\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.pose_landmarks,\n",
    "        mp_holistic.POSE_CONNECTIONS,\n",
    "        landmark_drawing_spec=mp_drawing_styles\n",
    "        .get_default_pose_landmarks_style())\n",
    "    \n",
    "    try:\n",
    "        \n",
    "      row = np.array([[res.x,res.y,res.z,res.visibility] for res in results.pose_landmarks.landmark]).flatten().tolist()\n",
    "      #position\n",
    "      X1 = pd.DataFrame([row],columns=landmarks[1:])\n",
    "      #lean\n",
    "      X2 = pd.DataFrame([row],columns=landmarks[1:])\n",
    "      #Stance\n",
    "      X3 = pd.DataFrame([row],columns=landmarks[1:])\n",
    "\n",
    "      body_language_class = model.predict(X1)[0]\n",
    "      body_language_prob = model.predict_proba(X1)[0]\n",
    "\n",
    "      lean_class = model1.predict(X2)[0]\n",
    "      lean_prob = model1.predict_proba(X2)[0]\n",
    "      # print(body_language_class,body_language_prob)\n",
    "\n",
    "      stance_class = model2.predict(X3)[0]\n",
    "      stance_prob = model2.predict_proba(X3)[0]\n",
    "      ################################################\n",
    "      if body_language_class == 'down' and body_language_prob[body_language_prob.argmax()] >= 0.7:\n",
    "          current_stage = 'down'\n",
    "          # print(\"huuuuuu\")\n",
    "      elif current_stage == 'down' and body_language_class == 'up' and body_language_prob[body_language_prob.argmax()] >= 0.7:\n",
    "          current_stage = 'up'\n",
    "          counter += 1\n",
    "          print(counter)\n",
    "          # print(\"this was hit!\")\n",
    "\n",
    "      ###################################################\n",
    "      if lean_class == 'neutral' and lean_prob[lean_prob.argmax()] >= 0.7:\n",
    "\n",
    "          current_lean_status = 'neutral'\n",
    "      elif lean_class == 'left' and lean_prob[lean_prob.argmax()] >= 0.7:\n",
    "          current_lean_status = 'left'\n",
    "      elif lean_class == 'right' and lean_prob[lean_prob.argmax()] >= 0.7:\n",
    "          current_lean_status = 'right'\n",
    "      \n",
    "\n",
    "      ######################################################\n",
    "      if stance_class == 'normal' and stance_prob[stance_prob.argmax()] >= 0.7:\n",
    "          current_stance_status = 'normal'\n",
    "          # print(\"huuuuuu\")\n",
    "      elif stance_class == 'wide' and stance_prob[stance_prob.argmax()] >= 0.7:\n",
    "          current_stance_status = 'wide'\n",
    "      elif stance_class == 'narrow' and stance_prob[stance_prob.argmax()] >= 0.7:\n",
    "          current_stance_status = 'narrow'  \n",
    "      ######################################################\n",
    "      #Extract Joint coordinates\n",
    "      my_landmarks = results.pose_landmarks.landmark\n",
    "      joint_coordinates = get_coordinates(my_landmarks)\n",
    "      #calculate joint angles\n",
    "      req_angles = get_all_angles(joint_coordinates)\n",
    "      req_angles = np.around(req_angles,2)\n",
    "      #display joint angles\n",
    "      for i in range(len(req_angles)):\n",
    "          # print(len(joint_coordinates))\n",
    "          cv2.putText(\n",
    "              image,\n",
    "              str(req_angles[i]),\n",
    "              tuple(np.multiply(joint_coordinates[i][1],[640, 480]).astype(int)),\n",
    "              cv2.FONT_HERSHEY_SIMPLEX,0.5,(255,255,255),2,cv2.LINE_AA\n",
    "          )\n",
    "      ###########################\n",
    "      #Display for Position\n",
    "      #Get status box\n",
    "      cv2.rectangle(image,(0,0),(250,60),(245,117,16),-1)\n",
    "\n",
    "      #Display class\n",
    "      cv2.putText(image,'CLASS'\n",
    "          ,(95,12),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,0,0),1,cv2.LINE_AA)\n",
    "      cv2.putText(image,body_language_class.split(' ')[0]\n",
    "      , (90,40),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2,cv2.LINE_AA)\n",
    "\n",
    "      #Display Probability\n",
    "      cv2.putText(image,'PROB',(15,12),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,0,0),1,cv2.LINE_AA)\n",
    "      cv2.putText(image,str(round(body_language_prob[np.argmax(body_language_prob)],2))\n",
    "      ,(10,40),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2,cv2.LINE_AA)\n",
    "\n",
    "      #Display Probability\n",
    "      cv2.putText(image,'COUNT',(180,12),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,0,0),1,cv2.LINE_AA)\n",
    "      cv2.putText(image,str(counter),(175,40),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2,cv2.LINE_AA)\n",
    "      ############################\n",
    "\n",
    "      ############################\n",
    "      # Display for Lean Status\n",
    "      # Get status box\n",
    "      cv2.rectangle(image,(250,0),(500,60),(245,117,16),-1)\n",
    "\n",
    "      #Display class\n",
    "      cv2.putText(image,'CLASS'\n",
    "          ,(95+ 250 ,12),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,0,0),1,cv2.LINE_AA)\n",
    "      cv2.putText(image,lean_class.split(' ')[0]\n",
    "      , (90 + 250 ,40),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2,cv2.LINE_AA)\n",
    "\n",
    "      #Display Probability\n",
    "      cv2.putText(image,'PROB',(15 + 250 ,12),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,0,0),1,cv2.LINE_AA)\n",
    "      cv2.putText(image,str(round(lean_prob[np.argmax(lean_prob)],2))\n",
    "      ,(10 + 250 ,40),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2,cv2.LINE_AA)\n",
    "      #############################\n",
    "\n",
    "      #############################\n",
    "      # Display for Stance Status\n",
    "      # Get status box\n",
    "      cv2.rectangle(image,(425,0),(640,60),(245,117,16),-1)\n",
    "\n",
    "      #Display class\n",
    "      cv2.putText(image,'CLASS'\n",
    "          ,(95+ 425 ,12),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,0,0),1,cv2.LINE_AA)\n",
    "      cv2.putText(image,stance_class.split(' ')[0]\n",
    "      , (90 + 425 ,40),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2,cv2.LINE_AA)\n",
    "\n",
    "      #Display Probability\n",
    "      cv2.putText(image,'PROB',(15 + 425 ,12),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,0,0),1,cv2.LINE_AA)\n",
    "      cv2.putText(image,str(round(stance_prob[np.argmax(stance_prob)],2))\n",
    "      ,(10 + 425 ,40),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2,cv2.LINE_AA)\n",
    "      #############################\n",
    "    \n",
    "    except Exception as e:\n",
    "        print('error',e)\n",
    "        pass\n",
    "\n",
    "    # Flip the image horizontally for a selfie-view display.\n",
    "    flipped_frame = cv2.flip(image, 0)\n",
    "\n",
    "    out.write(image)\n",
    "    cv2.imshow('MediaPipe Holistic',image)\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "      break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = [12.2342,5262.234234234]\n",
    "np.around(ff,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# initialize video capture object\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# define codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('turning_one.avi', fourcc, 20.0, (640, 480))\n",
    "\n",
    "# loop over frames from the video stream\n",
    "while cap.isOpened():\n",
    "    # read the next frame from the video stream\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret:\n",
    "        # write the frame to the output file\n",
    "        out.write(frame)\n",
    "\n",
    "        # display the resulting frame\n",
    "        cv2.imshow('frame', frame)\n",
    "\n",
    "        # exit if the user presses 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# release resources\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5,min_tracking_confidence=0.5) as holistic:\n",
    "  while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      # If loading a video, use 'break' instead of 'continue'.\n",
    "      continue\n",
    "\n",
    "    # To improve performance, optionally mark the image as not writeable to\n",
    "    # pass by reference.\n",
    "    image.flags.writeable = False\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = holistic.process(image)\n",
    "\n",
    "    # Draw landmark annotation on the image.\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.pose_landmarks,\n",
    "        mp_holistic.POSE_CONNECTIONS,\n",
    "        landmark_drawing_spec=mp_drawing_styles\n",
    "        .get_default_pose_landmarks_style())\n",
    "    \n",
    "    try:\n",
    "      ######################################################\n",
    "      #Extract Joint coordinates\n",
    "      my_landmarks = results.pose_landmarks.landmark\n",
    "      joint_coordinates = get_coordinates(my_landmarks)\n",
    "      #calculate joint angles\n",
    "      req_angles = get_all_angles(joint_coordinates)\n",
    "      req_angles = np.around(req_angles,2)\n",
    "      #display joint angles\n",
    "      for i in range(len(req_angles)):\n",
    "          # print(len(joint_coordinates))\n",
    "          cv2.putText(\n",
    "              image,\n",
    "              str(req_angles[i]),\n",
    "              tuple(np.multiply(joint_coordinates[i][1],[640, 480]).astype(int)),\n",
    "              cv2.FONT_HERSHEY_SIMPLEX,0.5,(255,255,255),2,cv2.LINE_AA\n",
    "          )\n",
    "      ###########################\n",
    "      #############################\n",
    "    \n",
    "    except Exception as e:\n",
    "        print('error',e)\n",
    "        pass\n",
    "\n",
    "    # Flip the image horizontally for a selfie-view display.\n",
    "    flipped_frame = cv2.flip(image, 0)\n",
    "\n",
    "    cv2.imshow('MediaPipe Holistic',image)\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "      break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5af138deeb868a1d9bcaaf787eaaebdba68e976f322cd0f21433a4579759f5f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
